{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..', '..', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/test_set_inference', '/home/ubuntu/anaconda3/lib/python39.zip', '/home/ubuntu/anaconda3/lib/python3.9', '/home/ubuntu/anaconda3/lib/python3.9/lib-dynload', '', '/home/ubuntu/anaconda3/lib/python3.9/site-packages', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/locket-0.2.1-py3.9.egg', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/extensions', '/home/ubuntu/.ipython', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/tmp/tmpm_an8fb8']\n",
      "['..', '..', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/test_set_inference', '/home/ubuntu/anaconda3/lib/python39.zip', '/home/ubuntu/anaconda3/lib/python3.9', '/home/ubuntu/anaconda3/lib/python3.9/lib-dynload', '', '/home/ubuntu/anaconda3/lib/python3.9/site-packages', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/locket-0.2.1-py3.9.egg', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/extensions', '/home/ubuntu/.ipython', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/tmp/tmpm_an8fb8', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "print(sys.path)\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/model\")\n",
    "print(sys.path)\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from model.aliproduct_model import ALIPRODUCT_CLIP\n",
    "from model.aliproduct_blip_model import ALIPRODUCT_BLIP\n",
    "from model.dataset import ALIPRODUCT_DATASET,prepare_data\n",
    "from model.CONFIG import CONFIG\n",
    "import faiss\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models = [\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/save_checkpoint_7_thu.pth\",\n",
    "#     \"model_size\":\"base\",\"model_name\":\"base_all_e_7\"},\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/Retrieval_aliproduct2_filtered/save_checkpoint_4.pth\",\n",
    "#     \"model_size\":\"base\",\"model_name\":\"base_1st_e_5\"},\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/base_2nd/base_2nd_e_5.pth\",\"model_size\":\"base\",\"model_name\":\"base_2nd_e_5\"},\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/base_3_itm/base_3rd_e_5.pth\",\n",
    "#     \"model_size\":\"base\",\"model_name\":\"base_3rd_e_5\"},\n",
    "#      {\"path\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_8.pth\",\"model_size\":\"large\",\"model_name\":\"large_all_e_9\"},\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/large_v4/save_checkpoint_4.pth\",\"model_size\":\"large\",\"model_name\":\"large_v1_e_5\"},\n",
    "#     ]\n",
    "\n",
    "\n",
    "models = [\n",
    "       {\"path\":\"/home/ubuntu/Desktop/large_v7/save_checkpoint_0.pth\",\"model_size\":\"large\",\"model_name\":\"large_v4_e_1\"}\n",
    "\n",
    "    ]\n",
    "\n",
    "\n",
    "test_df_path = \"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/test_set_inference/test.csv\"\n",
    "\n",
    "\n",
    "\n",
    "col_to_test =  \"test_caption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embed(caption_col,clip_model):\n",
    "    test_loader,df = prepare_data(test_df_path,\n",
    "    CONFIG.test_image_data_dir,\"test_imgs\"\n",
    "    ,\"test_imgs\",caption_col,160,preprocess,CONFIG.global_random_state,test=True,tokenize=False)\n",
    "    trainer = Trainer(gpus=1)\n",
    "    pred = trainer.predict(clip_model,test_loader)\n",
    "    full_pred = tuple(map(torch.concat, zip(*pred)))\n",
    "    image_embed,text_embed = full_pred\n",
    "\n",
    "    return image_embed,text_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embed(image_embed,text_embed,model_size:str,prefix:str):\n",
    "    img_embed_path = \"./image_embeds\"\n",
    "    txt_embed_path = \"./text_embeds\"\n",
    "    img_embed_filename = f\"img_emded-{model_size}-{prefix}.pkl\"\n",
    "    txt_embed_filename = f\"txt_emded-{model_size}-{prefix}.pkl\"\n",
    "    img_embed_full_path = f\"{img_embed_path}/{img_embed_filename}\"\n",
    "    txt_embed_full_path = f\"{txt_embed_path}/{txt_embed_filename}\"\n",
    "\n",
    "    image_embed_info_dict = {\"model_name\":prefix,\"model_size\":model_size,\"pair_name\":txt_embed_filename,\"embed\":image_embed}\n",
    "    text_embed_info_dict = {\"model_name\":prefix,\"model_size\":model_size,\"pair_name\":img_embed_filename,\"embed\":text_embed}\n",
    "\n",
    "    with open(img_embed_full_path,\"wb\") as file:\n",
    "        print(f\"saving img embed from {prefix} to {img_embed_full_path}\")\n",
    "        pickle.dump(image_embed_info_dict,file)\n",
    "    with open(txt_embed_full_path,\"wb\") as file:\n",
    "        print(f\"saving txt embed from {prefix} to {txt_embed_full_path}\")\n",
    "        pickle.dump(text_embed_info_dict,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting embeddings from large_v4_e_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from /home/ubuntu/Desktop/large_v7/save_checkpoint_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 313/313 [52:22<00:00, 10.04s/it]\n",
      "saving img embed from large_v4_e_1 to ./image_embeds/img_emded-large-large_v4_e_1.pkl\n",
      "saving txt embed from large_v4_e_1 to ./text_embeds/txt_emded-large-large_v4_e_1.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model in models:\n",
    "    print(f\"extracting embeddings from \"+model[\"model_name\"])\n",
    "    clip_model =ALIPRODUCT_BLIP(model[\"path\"],image_size,vit=model[\"model_size\"])\n",
    "    image_embed,text_embed = gen_embed(col_to_test,clip_model)\n",
    "    text_embed_no_null = text_embed[0:20000]\n",
    "    save_embed(image_embed,text_embed_no_null,model[\"model_size\"],model[\"model_name\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
