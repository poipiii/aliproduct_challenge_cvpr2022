{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..', '..', '..', '..', '..', '..', '..', '..', '..', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/test_set_inference', '/home/ubuntu/anaconda3/lib/python39.zip', '/home/ubuntu/anaconda3/lib/python3.9', '/home/ubuntu/anaconda3/lib/python3.9/lib-dynload', '', '/home/ubuntu/anaconda3/lib/python3.9/site-packages', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/locket-0.2.1-py3.9.egg', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/extensions', '/home/ubuntu/.ipython', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/tmp/tmpf_d004qf', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model']\n",
      "['..', '..', '..', '..', '..', '..', '..', '..', '..', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/test_set_inference', '/home/ubuntu/anaconda3/lib/python39.zip', '/home/ubuntu/anaconda3/lib/python3.9', '/home/ubuntu/anaconda3/lib/python3.9/lib-dynload', '', '/home/ubuntu/anaconda3/lib/python3.9/site-packages', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/locket-0.2.1-py3.9.egg', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/extensions', '/home/ubuntu/.ipython', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/tmp/tmpf_d004qf', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "print(sys.path)\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/model\")\n",
    "print(sys.path)\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from model.aliproduct_model import ALIPRODUCT_CLIP\n",
    "from model.aliproduct_blip_model import ALIPRODUCT_BLIP\n",
    "from model.dataset import ALIPRODUCT_DATASET,prepare_data\n",
    "from model.CONFIG import CONFIG\n",
    "import faiss\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "import json\n",
    "from autofaiss import build_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>test_caption</th>\n",
       "      <th>test_imgs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>feiyangg/LP Paragraph Style Electric Guitar Ti...</td>\n",
       "      <td>O1CN01r2RqZk1lJ4Sk2ZYaN_!!2200643934797.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Summer Lace White Shorts Pants Female Thin Ant...</td>\n",
       "      <td>O1CN01H0o2pi2HAtGC9rVWa_!!2206736519111.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>High-Quality Export 8MM with Adhesive Cork Cor...</td>\n",
       "      <td>O1CN01qeHW0Z1D3mRPfmNvM_!!161-0-lubanu.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>LED Ceiling Living Room Bedroom Corridor Corri...</td>\n",
       "      <td>O1CN01phQoOe25ljjN1lpLO_!!3082327567.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Disposable Cotton Cotton Female Thin Water Fac...</td>\n",
       "      <td>O1CN01Dzpgpr2FxJsUzqAdE_!!2963188946.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                       test_caption  \\\n",
       "0           0  feiyangg/LP Paragraph Style Electric Guitar Ti...   \n",
       "1           1  Summer Lace White Shorts Pants Female Thin Ant...   \n",
       "2           2  High-Quality Export 8MM with Adhesive Cork Cor...   \n",
       "3           3  LED Ceiling Living Room Bedroom Corridor Corri...   \n",
       "4           4  Disposable Cotton Cotton Female Thin Water Fac...   \n",
       "\n",
       "                                     test_imgs  \n",
       "0  O1CN01r2RqZk1lJ4Sk2ZYaN_!!2200643934797.jpg  \n",
       "1  O1CN01H0o2pi2HAtGC9rVWa_!!2206736519111.jpg  \n",
       "2   O1CN01qeHW0Z1D3mRPfmNvM_!!161-0-lubanu.jpg  \n",
       "3     O1CN01phQoOe25ljjN1lpLO_!!3082327567.jpg  \n",
       "4     O1CN01Dzpgpr2FxJsUzqAdE_!!2963188946.jpg  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df = pd.read_csv(\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/test_set_inference/test.csv\")\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embed_path = \"./image_embeds\"\n",
    "text_embed_path = \"./text_embeds\"\n",
    "image_embed_files = os.listdir(image_embed_path)\n",
    "text_embed_files = os.listdir(text_embed_path)\n",
    "image_embed_files.remove(\"val_set_embed\")\n",
    "text_embed_files.remove(\"val_set_embed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['txt_emded-large-large_v2_e_5.pkl',\n",
       " 'txt_emded-base-base_1st_e_5.pkl',\n",
       " 'txt_emded-base-base_3rd_e_5.pkl',\n",
       " 'txt_emded-base-base_2nd_e_5.pkl',\n",
       " 'txt_emded-base-base_all_e_7.pkl',\n",
       " 'txt_emded-base-base_5th_e_5.pkl',\n",
       " 'txt_emded-large-large_all_e_9.pkl',\n",
       " 'txt_emded-large-large_v3_e_1.pkl',\n",
       " 'txt_emded-base-base_4th_e_5.pkl',\n",
       " 'txt_emded-large-large_v1_e_5.pkl']"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_embed_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:01,  7.14it/s]\n"
     ]
    }
   ],
   "source": [
    "all_image_embed ={}\n",
    "all_text_embed = {}\n",
    "for i,t in tqdm(zip(image_embed_files,text_embed_files)):\n",
    "    with open(f\"{image_embed_path}/{i}\",\"rb\") as img_file:\n",
    "        img_embed = pickle.load(img_file)\n",
    "        all_image_embed[img_embed[\"model_name\"]] = img_embed[\"embed\"]\n",
    "    with open(f\"{text_embed_path}/{t}\",\"rb\") as txt_file:\n",
    "        txt_embed = pickle.load(txt_file)\n",
    "        all_text_embed[txt_embed[\"model_name\"]] = txt_embed[\"embed\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['large_v2_e_5', 'base_5th_e_5', 'base_all_e_7', 'base_2nd_e_5', 'base_1st_e_5', 'large_v1_e_5', 'base_3rd_e_5', 'base_4th_e_5', 'large_v3_e_1', 'large_all_e_9'])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_image_embed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['large_v2_e_5', 'base_1st_e_5', 'base_3rd_e_5', 'base_2nd_e_5', 'base_all_e_7', 'base_5th_e_5', 'large_all_e_9', 'large_v3_e_1', 'base_4th_e_5', 'large_v1_e_5'])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_embed.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_to_test = \"large_all_e_9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_to_ensamble = [\"base_5th_e_5\",\"large_v2_e_5\",\"large_v3_e_1\"]\n",
    "\n",
    "# embed_to_ensamble = list(all_text_embed.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_text_embed[\"base_all_e_7\"] = all_text_embed[\"base_all_e_7\"][0:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ensamble(all_image_embed,all_text_embed,embeds_to_ensamble,ensamble_strategy):\n",
    "#     ensamsables = {}\n",
    "#     img_embed_list = []\n",
    "#     text_embed_list = []\n",
    "#     for model_name in embeds_to_ensamble:\n",
    "#         img_embed_list.append(all_image_embed[model_name])\n",
    "#         text_embed_list.append(all_text_embed[model_name])\n",
    "#     if len(embeds_to_ensamble) > 1:\n",
    "#         if ensamble_strategy ==\"concat\":\n",
    "#             ensamsables[\"image\"] = torch.concat(img_embed_list,dim=1)\n",
    "#             ensamsables[\"text\"] = torch.concat(text_embed_list,dim=1)\n",
    "#         elif ensamble_strategy ==\"avg\":\n",
    "#             ensamsables[\"image\"] = torch.mean(torch.stack(img_embed_list))\n",
    "#             ensamsables[\"text\"] = torch.mean(torch.concat(text_embed_list))\n",
    "#         return ensamsables\n",
    "#     else:\n",
    "#             ensamsables[\"image\"] = img_embed_list\n",
    "#             ensamsables[\"text\"] = text_embed_list\n",
    "#             return ensamsables\n",
    "\n",
    "\n",
    "        \n",
    "# def get_ensamble(all_image_embed,all_text_embed,embeds_to_ensamble,ensamble_strategy):\n",
    "#     ensamsables = {}\n",
    "#     img_embed_list = []\n",
    "#     text_embed_list = []\n",
    "#     for model_name in embeds_to_ensamble:\n",
    "#         print(model_name)\n",
    "#         img_embed_list.append(all_image_embed[model_name])\n",
    "#         text_embed_list.append(all_text_embed[model_name])\n",
    "#     if len(embeds_to_ensamble) > 1:\n",
    "#         if ensamble_strategy ==\"concat\":\n",
    "#             ensamsables[\"image\"] = torch.concat(img_embed_list,dim=1)\n",
    "#             ensamsables[\"text\"] = torch.concat(text_embed_list,dim=1)\n",
    "#         elif ensamble_strategy ==\"avg\":\n",
    "#             ensamsables[\"image\"] = torch.mean(torch.stack(img_embed_list),0)\n",
    "#             ensamsables[\"text\"] = torch.mean(torch.stack(text_embed_list),0)\n",
    "#         else:\n",
    "#             ensamsables[\"image\"] = img_embed_list\n",
    "#             ensamsables[\"text\"] = text_embed_list\n",
    "#         return ensamsables\n",
    "#     else:\n",
    "#             ensamsables[\"image\"] = img_embed_list[0]\n",
    "#             ensamsables[\"text\"] = text_embed_list[0]\n",
    "#             return ensamsables\n",
    "\n",
    "\n",
    "\n",
    "def get_ensamble(all_image_embed,all_text_embed,embeds_to_ensamble,ensamble_strategy,all_weights):\n",
    "    ensamsables = {}\n",
    "    img_embed_list = []\n",
    "    text_embed_list = []\n",
    "    all_weights = all_weights\n",
    "    for model_name ,weights in zip(embeds_to_ensamble,range(len(all_weights))):\n",
    "        img_embed_list.append(all_image_embed[model_name]*all_weights[weights])\n",
    "        text_embed_list.append(all_text_embed[model_name]*all_weights[weights])\n",
    "    if len(embeds_to_ensamble) > 1:\n",
    "        if ensamble_strategy ==\"concat\":\n",
    "            ensamsables[\"image\"] = torch.concat(img_embed_list,dim=1)\n",
    "            ensamsables[\"text\"] = torch.concat(text_embed_list,dim=1)\n",
    "        elif ensamble_strategy ==\"avg\":\n",
    "            ensamsables[\"image\"] = torch.mean(torch.stack(img_embed_list),0)\n",
    "            ensamsables[\"text\"] = torch.mean(torch.stack(text_embed_list),0)\n",
    "        else:\n",
    "            ensamsables[\"image\"] = img_embed_list\n",
    "            ensamsables[\"text\"] = text_embed_list\n",
    "        return ensamsables\n",
    "    else:\n",
    "            ensamsables[\"image\"] = img_embed_list[0]\n",
    "            ensamsables[\"text\"] = text_embed_list[0]\n",
    "            return ensamsables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensamsables = get_ensamble(all_image_embed,all_text_embed,embed_to_ensamble,\"concat\")\n",
    "\n",
    "ensamsables = get_ensamble(all_image_embed,all_text_embed,embed_to_ensamble,\"concat\",[1.5,2,0.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_preds(ensamsables,score=False,df=None):\n",
    "#     image_embed = ensamsables[\"image\"]\n",
    "#     text_embed = ensamsables[\"text\"]\n",
    "#     faiss_index = faiss.IndexFlatIP(image_embed.shape[1])\n",
    "#     faiss_index.add(image_embed.numpy().astype(np.float32))\n",
    "#     top5_k_e,top5_k_y_pred = faiss_index.search(text_embed.numpy().astype(np.float32),5)\n",
    "#     top10_k_e,top10_k_y_pred = faiss_index.search(text_embed.numpy().astype(np.float32),10)\n",
    "#     if score:\n",
    "#         top5_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top5_k_y_pred)])\n",
    "#         top10_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top10_k_y_pred)])\n",
    "#         top5_acc = top5_preds[top5_preds ==1].shape[0] / top5_preds.shape[0]\n",
    "#         print(f\"top_5 accuracy:{top5_acc}\")\n",
    "#         top10_acc = top10_preds[top10_preds ==1].shape[0] / top10_preds.shape[0]\n",
    "#         avg_acc = (top5_acc+top10_acc)/2\n",
    "#         print(f\"top_5 accuracy:{top10_acc}\")\n",
    "#         print(f\"avg accuracy:{avg_acc}\")\n",
    "#         return top5_acc,top10_acc,avg_acc\n",
    "#     else:\n",
    "#         return top5_k_y_pred,top10_k_y_pred,top5_k_e,top10_k_e\n",
    "\n",
    "\n",
    "def get_preds(ensamsables,score=False,df=None,use_faiss=True,auto_faiss=False,max_ensamble=False,sharpen_weight = 1):\n",
    "    image_embed = ensamsables[\"image\"]\n",
    "    text_embed = ensamsables[\"text\"]\n",
    "    if use_faiss:\n",
    "        faiss_index = faiss.IndexFlatIP(image_embed.shape[1])\n",
    "        faiss_index.add(image_embed.numpy().astype(np.float32))\n",
    "        top5_k_e,top5_k_y_pred = faiss_index.search(text_embed.numpy().astype(np.float32),5)\n",
    "        top10_k_e,top10_k_y_pred = faiss_index.search(text_embed.numpy().astype(np.float32),10)\n",
    "    elif auto_faiss:\n",
    "        auto_faiss_index,index_infos  = build_index(image_embed.numpy().astype(np.float32), save_on_disk=False)\n",
    "        top5_k_e,top5_k_y_pred = auto_faiss_index.search(text_embed.numpy().astype(np.float32),5)\n",
    "        top10_k_e,top10_k_y_pred = auto_faiss_index.search(text_embed.numpy().astype(np.float32),10)\n",
    "\n",
    "    elif max_ensamble:\n",
    "        all_top10_cosine_sim  =[]\n",
    "        all_top5_cosine_sim  =[]\n",
    "        all_top10_pred_idx  =[]\n",
    "        all_top5_pred_idx  =[]\n",
    "        for text,image in zip(text_embed,image_embed):\n",
    "            faiss_index = faiss.IndexFlatIP(image.shape[1])\n",
    "            faiss_index.add(image.numpy().astype(np.float32))\n",
    "            top5_cosine_sim,top5_pred_idx = faiss_index.search(text.numpy().astype(np.float32),5)\n",
    "            top10_cosine_sim,top10_pred_idx = faiss_index.search(text.numpy().astype(np.float32),10)\n",
    "            all_top10_cosine_sim.append(top10_cosine_sim)\n",
    "            all_top5_cosine_sim.append(top5_cosine_sim) \n",
    "            all_top10_pred_idx.append(top10_pred_idx)\n",
    "            all_top5_pred_idx.append(top5_pred_idx)\n",
    "        all_top10_cosine_sim  = np.concatenate(all_top10_cosine_sim,axis=1)\n",
    "        all_top5_cosine_sim  = np.concatenate(all_top5_cosine_sim,axis=1)\n",
    "        all_top10_pred_idx  = np.concatenate(all_top10_pred_idx,axis=1)\n",
    "        all_top5_pred_idx  = np.concatenate(all_top5_pred_idx,axis=1)\n",
    "        top5_k_y_pred = []\n",
    "        top10_k_y_pred = []\n",
    "        for t5,t10,t5_i,t10_i in zip(all_top5_cosine_sim,all_top10_cosine_sim,all_top5_pred_idx,all_top10_pred_idx):\n",
    "            top5_k_y_pred.append(t5_i[np.argpartition(t5,-5)[-5:]])\n",
    "            top10_k_y_pred.append(t10_i[np.argpartition(t10,-10)[-10:]])\n",
    "        print(top10_k_y_pred)\n",
    "\n",
    "    else:\n",
    "        all_cosine_sim  =[]\n",
    "        all_weights = [0.3,0.5,0.2]\n",
    "        for text,image,w_i in zip(text_embed,image_embed,range(len(all_weights))):\n",
    "            cosine_sim = text @ image.T \n",
    "            all_cosine_sim.append(cosine_sim * all_weights[w_i])\n",
    "            del cosine_sim\n",
    "        # ensamble_cosine_sim = torch.mean(torch.stack(all_cosine_sim),dim=0) \n",
    "        # ensamble_cosine_sim = 1 / torch.mean(1. / (torch.stack(all_cosine_sim)) +0.0001,dim=0) \n",
    "        ensamble_cosine_sim = torch.prod(torch.stack(all_cosine_sim),dim=0)**1/3\n",
    "        top5_k_e,top5_k_y_pred = torch.topk(ensamble_cosine_sim,5)\n",
    "        top10_k_e,top10_k_y_pred = torch.topk(ensamble_cosine_sim,10)\n",
    "    if score:\n",
    "            top5_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top5_k_y_pred)])\n",
    "            top10_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top10_k_y_pred)])\n",
    "            top5_acc = top5_preds[top5_preds ==1].shape[0] / top5_preds.shape[0]\n",
    "            top10_acc = top10_preds[top10_preds ==1].shape[0] / top10_preds.shape[0]\n",
    "            avg_acc = (top5_acc+top10_acc)/2\n",
    "            return top5_acc,top10_acc,avg_acc\n",
    "    else:\n",
    "        return top5_k_y_pred,top10_k_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top5_pred_idx,top10_pred_idx,top5_k_e,top10_k_e = get_preds(ensamsables,score=False,df=clean_df)\n",
    "top5_pred_idx,top10_pred_idx = get_preds(ensamsables,score=False,df=clean_df,use_faiss=True,auto_faiss=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remap_pred_to_name(top5_pred_idx,top10_pred_idx,image_array):\n",
    "    top5_pred = [list(image_array[p]) for p in top5_pred_idx]\n",
    "    top10_pred =  [list(image_array[p]) for p in top10_pred_idx]\n",
    "    return top5_pred,top10_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5_pred,top10_pred =remap_pred_to_name(top5_pred_idx,top10_pred_idx,clean_df[\"test_imgs\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_json(top5_pred,top10_pred,caption_array,result_dir,result_file_name):\n",
    "    results = []\n",
    "    for c,t5,t10 in tqdm(zip(caption_array,top5_pred,top10_pred)):\n",
    "        result_dict = {}\n",
    "        result_dict[\"caption\"] = c\n",
    "        result_dict[\"top5\"] = t5\n",
    "        result_dict[\"top10\"] = t10\n",
    "        results.append(result_dict)\n",
    "    print(\"checking results format\")\n",
    "    print(results[0])\n",
    "    with open(f\"{result_dir}/{result_file_name}.json\",\"w\") as results_file:\n",
    "        json.dump(results,results_file)\n",
    "    with open(f\"{result_dir}/{result_file_name}.json\",\"r\") as results_file:\n",
    "        check_results = json.load(results_file)\n",
    "    print(\"checking results format of saved file\")\n",
    "    print(check_results[-1])\n",
    "    return check_results\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20000it [00:00, 753226.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking results format\n",
      "{'caption': 'feiyangg/LP Paragraph Style Electric Guitar Tiger Veneer Factory Direct Color Can Be Customized', 'top5': ['TB25xAzdCGI.eBjSspcXXcVjFXa_!!754885367.jpg', 'O1CN011pW88rTaqfnNIi1_!!754885367.jpg', 'O1CN01p912eO1lKuB1TiTl6_!!2207916444801.png', 'O1CN01Dj2czX1oWlFVAZ05k_!!2207737885233.jpg', 'O1CN01XJH6iU1lKuBgk0ndT_!!2207916444801.jpg'], 'top10': ['TB25xAzdCGI.eBjSspcXXcVjFXa_!!754885367.jpg', 'O1CN011pW88rTaqfnNIi1_!!754885367.jpg', 'O1CN01p912eO1lKuB1TiTl6_!!2207916444801.png', 'O1CN01Dj2czX1oWlFVAZ05k_!!2207737885233.jpg', 'O1CN01XJH6iU1lKuBgk0ndT_!!2207916444801.jpg', 'O1CN016zxVN01qPYIlq8the_!!0-item_pic.jpg', 'O1CN01S8ewME1m1UwiK89Pr_!!1973994894.jpg', 'TB1C3GMkUD1gK0jSZFGL6Td3FXa', 'TB2lylSaQ.OyuJjSszhXXbZbVXa_!!20295181.jpg', 'O1CN01ha825u1ctPVv8XwDP_!!2211470413658-0-picasso.jpg']}\n",
      "checking results format of saved file\n",
      "{'caption': 'mrs harmay foods Handmade Cantonese Barbecued Pork Bag 390g/Bag', 'top5': ['O1CN01472J632BQ6ioi13iS_!!2680068332.jpg', 'O1CN019zeHtc2BQ6VwvUSxE_!!2680068332.jpg', 'O1CN01tBoUtb2BQ6fzmLjAK_!!2680068332.jpg', 'O1CN01Eu7xDm2BQ6e9Q3DyO_!!2680068332.jpg', 'O1CN012BQ6UYTBqKChElK_!!2680068332.jpg'], 'top10': ['O1CN01472J632BQ6ioi13iS_!!2680068332.jpg', 'O1CN019zeHtc2BQ6VwvUSxE_!!2680068332.jpg', 'O1CN01tBoUtb2BQ6fzmLjAK_!!2680068332.jpg', 'O1CN01Eu7xDm2BQ6e9Q3DyO_!!2680068332.jpg', 'O1CN012BQ6UYTBqKChElK_!!2680068332.jpg', 'O1CN01rvQeuB1sKAt16EWcs_!!263685747.jpg', 'O1CN01y0oi5E2BQ6iiVSuV0_!!2680068332.jpg', 'O1CN013wLXOX2BQ6hmvVwEE_!!2680068332.jpg', 'O1CN018QJNrn2BQ6gAeJfPe_!!2680068332.jpg', 'O1CN01aK0SJY2BQ6UpSTVgJ_!!2680068332.jpg']}\n"
     ]
    }
   ],
   "source": [
    "saved_result = results_to_json(top5_pred,top10_pred,clean_df[\"test_caption\"].values[0:20000],\"./submissions\",\"test_ensamble_top3_w_v2_8_6_22\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(saved_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'caption': 'feiyangg/LP Paragraph Style Electric Guitar Tiger Veneer Factory Direct Color Can Be Customized',\n",
       "  'top5': ['TB25xAzdCGI.eBjSspcXXcVjFXa_!!754885367.jpg',\n",
       "   'O1CN011pW88rTaqfnNIi1_!!754885367.jpg',\n",
       "   'O1CN01p912eO1lKuB1TiTl6_!!2207916444801.png',\n",
       "   'O1CN01Dj2czX1oWlFVAZ05k_!!2207737885233.jpg',\n",
       "   'O1CN01XJH6iU1lKuBgk0ndT_!!2207916444801.jpg'],\n",
       "  'top10': ['TB25xAzdCGI.eBjSspcXXcVjFXa_!!754885367.jpg',\n",
       "   'O1CN011pW88rTaqfnNIi1_!!754885367.jpg',\n",
       "   'O1CN01p912eO1lKuB1TiTl6_!!2207916444801.png',\n",
       "   'O1CN01Dj2czX1oWlFVAZ05k_!!2207737885233.jpg',\n",
       "   'O1CN01XJH6iU1lKuBgk0ndT_!!2207916444801.jpg',\n",
       "   'O1CN016zxVN01qPYIlq8the_!!0-item_pic.jpg',\n",
       "   'O1CN01S8ewME1m1UwiK89Pr_!!1973994894.jpg',\n",
       "   'TB1C3GMkUD1gK0jSZFGL6Td3FXa',\n",
       "   'TB2lylSaQ.OyuJjSszhXXbZbVXa_!!20295181.jpg',\n",
       "   'O1CN01ha825u1ctPVv8XwDP_!!2211470413658-0-picasso.jpg']}]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_result[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[27694, 47178, 35939, 40609, 42193],\n",
       "       [23383, 19321,  5591, 44309, 16976],\n",
       "       [31777, 10273, 21649, 34609, 16947],\n",
       "       ...,\n",
       "       [24440,  7107, 18232, 14769, 43062],\n",
       "       [12274, 39630,  4278, 18451, 20602],\n",
       "       [35529, 43082,  6883, 18811, 35923]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top5_pred_idx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
