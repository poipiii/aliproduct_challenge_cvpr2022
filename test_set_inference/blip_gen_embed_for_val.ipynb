{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['..', '..', '..', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/test_set_inference', '/home/ubuntu/anaconda3/lib/python39.zip', '/home/ubuntu/anaconda3/lib/python3.9', '/home/ubuntu/anaconda3/lib/python3.9/lib-dynload', '', '/home/ubuntu/anaconda3/lib/python3.9/site-packages', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/locket-0.2.1-py3.9.egg', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/extensions', '/home/ubuntu/.ipython', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/tmp/tmp9u_12xil', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model']\n",
      "['..', '..', '..', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/test_set_inference', '/home/ubuntu/anaconda3/lib/python39.zip', '/home/ubuntu/anaconda3/lib/python3.9', '/home/ubuntu/anaconda3/lib/python3.9/lib-dynload', '', '/home/ubuntu/anaconda3/lib/python3.9/site-packages', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/locket-0.2.1-py3.9.egg', '/home/ubuntu/anaconda3/lib/python3.9/site-packages/IPython/extensions', '/home/ubuntu/.ipython', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/tmp/tmp9u_12xil', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model', '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "print(sys.path)\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/model\")\n",
    "print(sys.path)\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from model.aliproduct_model import ALIPRODUCT_CLIP\n",
    "from model.aliproduct_blip_model import ALIPRODUCT_BLIP\n",
    "from model.dataset import ALIPRODUCT_DATASET,prepare_data\n",
    "from model.CONFIG import CONFIG\n",
    "import faiss\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/data/val_data_map.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>caption</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Tanabata Valentine's Day Shenzhen Bao'an M Nan...</td>\n",
       "      <td>O1CN01cSoTwD1spJos7ZSF6_!!0-item_pic.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Children's Toys Little Girl over 6 Years Old G...</td>\n",
       "      <td>O1CN01iI5sGv1vIkV5dfICu_!!0-item_pic.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sm qing qu Alternative Sex between Men and Wom...</td>\n",
       "      <td>O1CN01xnnyaz248W0n5le2q_!!131027346.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Traditional Chinese Painting Burnin' Up Yingke...</td>\n",
       "      <td>O1CN01lfHuuA1D3K2MEM63p_!!160-0-lubanu.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>lgnace Lee Men's Jeans Thick Section Distresse...</td>\n",
       "      <td>TB23FV3afBNTKJjy0FdXXcPpVXa_!!1944606990.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                            caption  \\\n",
       "0           0  Tanabata Valentine's Day Shenzhen Bao'an M Nan...   \n",
       "1           1  Children's Toys Little Girl over 6 Years Old G...   \n",
       "2           2  sm qing qu Alternative Sex between Men and Wom...   \n",
       "3           3  Traditional Chinese Painting Burnin' Up Yingke...   \n",
       "4           4  lgnace Lee Men's Jeans Thick Section Distresse...   \n",
       "\n",
       "                                        product  \n",
       "0      O1CN01cSoTwD1spJos7ZSF6_!!0-item_pic.jpg  \n",
       "1      O1CN01iI5sGv1vIkV5dfICu_!!0-item_pic.jpg  \n",
       "2       O1CN01xnnyaz248W0n5le2q_!!131027346.jpg  \n",
       "3    O1CN01lfHuuA1D3K2MEM63p_!!160-0-lubanu.jpg  \n",
       "4  TB23FV3afBNTKJjy0FdXXcPpVXa_!!1944606990.jpg  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>caption</th>\n",
       "      <th>product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>49995</td>\n",
       "      <td>The Office for the Narrow Bed off K Frame If, ...</td>\n",
       "      <td>TB1hHR5JW61gK0jSZFlq6xDKFXa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>49996</td>\n",
       "      <td>Thick Aluminum xue ping guo F Milk Pot Wooden ...</td>\n",
       "      <td>TB1cVeRzkL0gK0jSZFtq6xQCXXa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>49997</td>\n",
       "      <td>Thick Warm Crystal Velvet Quilted Bed Skirt Pi...</td>\n",
       "      <td>O1CN01r4aAWL1Cluyu7h3kf_!!122-0-lubanu.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>49998</td>\n",
       "      <td>Plus Velvet Thickening Universal Elastic Wood ...</td>\n",
       "      <td>TB13sAjLAL0gK0jSZFAq6AA9pXa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>49999</td>\n",
       "      <td>M2 SKH51 M35 SKH55 M42 High-Speed Steel Bar fd...</td>\n",
       "      <td>O1CN01Q4ztF71RM0mnnKpXE_!!2200742622096.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                            caption  \\\n",
       "49995       49995  The Office for the Narrow Bed off K Frame If, ...   \n",
       "49996       49996  Thick Aluminum xue ping guo F Milk Pot Wooden ...   \n",
       "49997       49997  Thick Warm Crystal Velvet Quilted Bed Skirt Pi...   \n",
       "49998       49998  Plus Velvet Thickening Universal Elastic Wood ...   \n",
       "49999       49999  M2 SKH51 M35 SKH55 M42 High-Speed Steel Bar fd...   \n",
       "\n",
       "                                           product  \n",
       "49995                  TB1hHR5JW61gK0jSZFlq6xDKFXa  \n",
       "49996                  TB1cVeRzkL0gK0jSZFtq6xQCXXa  \n",
       "49997   O1CN01r4aAWL1Cluyu7h3kf_!!122-0-lubanu.jpg  \n",
       "49998                  TB13sAjLAL0gK0jSZFAq6AA9pXa  \n",
       "49999  O1CN01Q4ztF71RM0mnnKpXE_!!2200742622096.jpg  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col_to_test =  [c for c in clean_df.columns.values if \"caption\" in c] + [c for c in clean_df.columns.values if \"step\" in c]\n",
    "col_to_test =  \"caption\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_embed(caption_col,clip_model):\n",
    "    test_loader,df = prepare_data(\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/data/val_data_map.csv\",\n",
    "    CONFIG.test_image_data_dir,\"val_imgs\"\n",
    "    ,\"product\",caption_col,160,preprocess,CONFIG.global_random_state,test=True,tokenize=False)\n",
    "    trainer = Trainer(accelerator=\"gpu\", devices=[2])\n",
    "    pred = trainer.predict(clip_model,test_loader)\n",
    "    full_pred = tuple(map(torch.concat, zip(*pred)))\n",
    "    image_embed,text_embed = full_pred\n",
    "\n",
    "    return image_embed,text_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embed(image_embed,text_embed,model_size:str,prefix:str):\n",
    "    img_embed_path = \"./image_embeds/val_set_embed\"\n",
    "    txt_embed_path = \"./text_embeds/val_set_embed\"\n",
    "    img_embed_filename = f\"img_emded-{model_size}-{prefix}.pkl\"\n",
    "    txt_embed_filename = f\"txt_emded-{model_size}-{prefix}.pkl\"\n",
    "    img_embed_full_path = f\"{img_embed_path}/{img_embed_filename}\"\n",
    "    txt_embed_full_path = f\"{txt_embed_path}/{txt_embed_filename}\"\n",
    "\n",
    "    image_embed_info_dict = {\"model_name\":prefix,\"model_size\":model_size,\"pair_name\":txt_embed_filename,\"embed\":image_embed}\n",
    "    text_embed_info_dict = {\"model_name\":prefix,\"model_size\":model_size,\"pair_name\":img_embed_filename,\"embed\":text_embed}\n",
    "\n",
    "    with open(img_embed_full_path,\"wb\") as file:\n",
    "        print(f\"saving img embed from {prefix} to {img_embed_full_path}\")\n",
    "        pickle.dump(image_embed_info_dict,file)\n",
    "    with open(txt_embed_full_path,\"wb\") as file:\n",
    "        print(f\"saving txt embed from {prefix} to {txt_embed_full_path}\")\n",
    "        pickle.dump(text_embed_info_dict,file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting embeddings from large_v4_e_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from /home/ubuntu/Desktop/large_v7/save_checkpoint_0.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 313/313 [52:20<00:00, 10.03s/it]\n",
      "saving img embed from large_v4_e_1 to ./image_embeds/val_set_embed/img_emded-large-large_v4_e_1.pkl\n",
      "saving txt embed from large_v4_e_1 to ./text_embeds/val_set_embed/txt_emded-large-large_v4_e_1.pkl\n"
     ]
    }
   ],
   "source": [
    "# models = [\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/save_checkpoint_7_thu.pth\",\n",
    "#     \"model_size\":\"base\",\"model_name\":\"base_all_e_7\"},\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/Retrieval_aliproduct2_filtered/save_checkpoint_4.pth\",\n",
    "#     \"model_size\":\"base\",\"model_name\":\"base_1st_e_5\"},\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/base_2nd/base_2nd_e_5.pth\",\"model_size\":\"base\",\"model_name\":\"base_2nd_e_5\"},\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/base_3_itm/base_3rd_e_5.pth\",\n",
    "#     \"model_size\":\"base\",\"model_name\":\"base_3rd_e_5\"},\n",
    "#      {\"path\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_8.pth\",\"model_size\":\"large\",\"model_name\":\"large_all_e_9\"},\n",
    "#     {\"path\":\"/home/ubuntu/Desktop/large_v4/save_checkpoint_4.pth\",\"model_size\":\"large\",\"model_name\":\"large_v1_e_5\"},\n",
    "#     ]\n",
    "\n",
    "\n",
    "models = [\n",
    "       {\"path\":\"/home/ubuntu/Desktop/large_v7/save_checkpoint_0.pth\",\"model_size\":\"large\",\"model_name\":\"large_v4_e_1\"}\n",
    "    ]\n",
    "\n",
    "for model in models:\n",
    "    print(f\"extracting embeddings from \"+model[\"model_name\"])\n",
    "    clip_model =ALIPRODUCT_BLIP(model[\"path\"],image_size,vit=model[\"model_size\"])\n",
    "    image_embed,text_embed = gen_embed(col_to_test,clip_model)\n",
    "    save_embed(image_embed,text_embed,model[\"model_size\"],model[\"model_name\"])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
