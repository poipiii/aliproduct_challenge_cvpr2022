{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "print(module_path)\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/model\")\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from model.aliproduct_model import ALIPRODUCT_CLIP\n",
    "from model.aliproduct_blip_model import ALIPRODUCT_BLIP\n",
    "from model.dataset import ALIPRODUCT_DATASET,prepare_data\n",
    "from model.CONFIG import CONFIG\n",
    "import faiss\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''Edit this to specify checkpoints of models you want to test \n",
    "defined as key value pair of {\"checpoint name/identifier:\"full path to checkpoint\"}'''\n",
    "\n",
    "checkpoints = {\"base\":\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth\",\n",
    "\"epoch_1\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_0.pth\",\n",
    "\"epoch_2\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_1.pth\",\n",
    "\"epoch_3\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_2.pth\",\n",
    "\"epoch_4\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_3.pth\",\n",
    "\"epoch_5\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_4.pth\",\n",
    "\"epoch_6\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_5.pth\",\n",
    "\"epoch_7\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_6.pth\",\n",
    "\"epoch_8\":\"/home/ubuntu/Desktop/Retrieval_aliproduct2_blip_large/save_checkpoint_7.pth\"}\n",
    "\n",
    "#Edit this to specify the augmented/original captions you want to test the model checkpoints on \n",
    "caption_to_test =  [\"caption\"]\n",
    "\n",
    "\n",
    "#path to validation data after preporcessing into csv file format \n",
    "dataframe_path = \"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/data/val_data_prompt_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_checkpoint(caption_col,clip_model,dataframe_path):\n",
    "    test_loader,df = prepare_data(dataframe_path,\n",
    "    CONFIG.test_image_data_dir,CONFIG.test_image_data_folder\n",
    "    ,CONFIG.test_image_col,caption_col,100,preprocess,CONFIG.global_random_state,test=True,tokenize=False)\n",
    "    trainer = Trainer(gpus=1)\n",
    "    pred = trainer.predict(clip_model,test_loader)\n",
    "    full_pred = tuple(map(torch.concat, zip(*pred)))\n",
    "    image_embed,text_embed = full_pred\n",
    "    # print(image_embed.size())\n",
    "    # print(text_embed.size())\n",
    "    faiss_index = faiss.IndexFlatIP(256)\n",
    "    faiss_index.add(image_embed.numpy().astype(np.float32))\n",
    "    top5_k_e,top5_k_y_pred = faiss_index.search(text_embed.numpy().astype(np.float32),5)\n",
    "    top10_k_e,top10_k_y_pred = faiss_index.search(text_embed.numpy().astype(np.float32),10)\n",
    "    top5_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top5_k_y_pred)])\n",
    "    print(caption_col,\"num correct pred\",sum(top5_preds))\n",
    "    top10_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top10_k_y_pred)])\n",
    "    print(caption_col,\"num correct pred\",sum(top10_preds))\n",
    "    top5_acc = top5_preds[top5_preds ==1].shape[0] / top5_preds.shape[0]\n",
    "    top10_acc = top10_preds[top10_preds ==1].shape[0] / top10_preds.shape[0]\n",
    "    return top5_acc,top10_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "top5_acc_preds = []\n",
    "top10_acc_preds = []\n",
    "captions_tested = []\n",
    "for cap in caption_to_test:\n",
    "    for checkpoint in checkpoints.keys():\n",
    "        clip_model =ALIPRODUCT_BLIP(checkpoints[checkpoint],image_size,vit=\"large\")\n",
    "        top5_acc,top10_acc = test_checkpoint(cap,clip_model,dataframe_path)\n",
    "        top5_acc_preds.append(top5_acc)\n",
    "        top10_acc_preds.append(top10_acc)\n",
    "        captions_tested.append(cap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = {\"model_checkpoint\":checkpoints.keys(),\"top_5\":top5_acc_preds,\"top_10\":top10_acc_preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
