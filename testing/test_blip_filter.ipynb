{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/model\")\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from model.aliproduct_model import ALIPRODUCT_CLIP\n",
    "from model.aliproduct_blip_model import ALIPRODUCT_BLIP\n",
    "from model.dataset import ALIPRODUCT_DATASET,prepare_data\n",
    "from model.CONFIG import CONFIG\n",
    "import faiss\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/eda/train_data_v1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>product</th>\n",
       "      <th>image_file_path</th>\n",
       "      <th>image_parent_folder</th>\n",
       "      <th>caption_word_count</th>\n",
       "      <th>img_ext</th>\n",
       "      <th>full_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Z-Couple Shook His Head Ornaments Doll Car Dec...</td>\n",
       "      <td>O1CN01ogOt7d1DHyjGtaaWT_!!192-0-lubanu.jpg</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "      <td>train_text_img_pairs_0/train_text_img_pairs_0_...</td>\n",
       "      <td>20</td>\n",
       "      <td>jpg</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Z Remote Anti-Theft Door Wireless Door Anti-Th...</td>\n",
       "      <td>O1CN01eFgq841CxMtYCpqWF_!!147-0-lubanu.jpg</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "      <td>train_text_img_pairs_0/train_text_img_pairs_0_...</td>\n",
       "      <td>14</td>\n",
       "      <td>jpg</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Z Qinghai qing liang Xidan Tip outside Microme...</td>\n",
       "      <td>O1CN01fXCPeA1SzgmI1mVLf_!!2207662892318.jpg</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "      <td>train_text_img_pairs_0/train_text_img_pairs_0_...</td>\n",
       "      <td>16</td>\n",
       "      <td>jpg</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Z Magic Incognito Clothes Stack Pants Multi-La...</td>\n",
       "      <td>O1CN01qgixtq1BtPqrInOuX_!!3-0-lubanu.jpg</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "      <td>train_text_img_pairs_0/train_text_img_pairs_0_...</td>\n",
       "      <td>19</td>\n",
       "      <td>jpg</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>z ge zi yong pin yong ju ge zi zhuang men tie ...</td>\n",
       "      <td>TB1ZqfODEY1gK0jSZFCq6AwqXXa</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "      <td>train_text_img_pairs_0/train_text_img_pairs_0_...</td>\n",
       "      <td>30</td>\n",
       "      <td>qXXa</td>\n",
       "      <td>/home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             caption  \\\n",
       "0  Z-Couple Shook His Head Ornaments Doll Car Dec...   \n",
       "1  Z Remote Anti-Theft Door Wireless Door Anti-Th...   \n",
       "2  Z Qinghai qing liang Xidan Tip outside Microme...   \n",
       "3  Z Magic Incognito Clothes Stack Pants Multi-La...   \n",
       "4  z ge zi yong pin yong ju ge zi zhuang men tie ...   \n",
       "\n",
       "                                       product  \\\n",
       "0   O1CN01ogOt7d1DHyjGtaaWT_!!192-0-lubanu.jpg   \n",
       "1   O1CN01eFgq841CxMtYCpqWF_!!147-0-lubanu.jpg   \n",
       "2  O1CN01fXCPeA1SzgmI1mVLf_!!2207662892318.jpg   \n",
       "3     O1CN01qgixtq1BtPqrInOuX_!!3-0-lubanu.jpg   \n",
       "4                  TB1ZqfODEY1gK0jSZFCq6AwqXXa   \n",
       "\n",
       "                                     image_file_path  \\\n",
       "0  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...   \n",
       "1  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...   \n",
       "2  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...   \n",
       "3  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...   \n",
       "4  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...   \n",
       "\n",
       "                                 image_parent_folder  caption_word_count  \\\n",
       "0  train_text_img_pairs_0/train_text_img_pairs_0_...                  20   \n",
       "1  train_text_img_pairs_0/train_text_img_pairs_0_...                  14   \n",
       "2  train_text_img_pairs_0/train_text_img_pairs_0_...                  16   \n",
       "3  train_text_img_pairs_0/train_text_img_pairs_0_...                  19   \n",
       "4  train_text_img_pairs_0/train_text_img_pairs_0_...                  30   \n",
       "\n",
       "  img_ext                                          full_path  \n",
       "0     jpg  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...  \n",
       "1     jpg  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...  \n",
       "2     jpg  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...  \n",
       "3     jpg  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...  \n",
       "4    qXXa  /home/ubuntu/Desktop/CVPR 2022 AliProducts Cha...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 384\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature(caption_col,clip_model):\n",
    "    test_loader,df = prepare_data(\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/eda/train_data_v1.csv\",\n",
    "    CONFIG.test_image_data_dir,CONFIG.test_image_data_folder\n",
    "    ,CONFIG.image_col,caption_col,CONFIG.test_batch_size,preprocess,CONFIG.global_random_state,test=False,use_all=True,tokenize=False)\n",
    "    trainer = Trainer(gpus=1)\n",
    "    pred = trainer.predict(clip_model,test_loader)\n",
    "    full_pred = tuple(map(torch.concat, zip(*pred)))\n",
    "    image_embed,text_embed = full_pred    \n",
    "    return image_embed,text_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from /home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/save_checkpoint_5_tues.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0:   4%|‚ñç         | 1109/25624 [1:06:21<24:26:53,  3.59s/it]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "zip() argument after * must be an iterable, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25476/4282370455.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/save_checkpoint_5_tues.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclip_model\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mALIPRODUCT_BLIP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_to_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_25476/2736796839.py\u001b[0m in \u001b[0;36mtest_feature\u001b[0;34m(caption_col, clip_model)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclip_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfull_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mimage_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage_embed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext_embed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: zip() argument after * must be an iterable, not NoneType"
     ]
    }
   ],
   "source": [
    "col_to_test =  \"caption\"\n",
    "checkpoint = \"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/save_checkpoint_5_tues.pth\"\n",
    "clip_model =ALIPRODUCT_BLIP(checkpoint,image_size)\n",
    "image_embed,text_embed = test_feature(col_to_test,clip_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50000it [00:00, 144393.37it/s]\n"
     ]
    }
   ],
   "source": [
    "single_pair_cosine = np.array(list((txt @ img.T).item() for txt ,img in tqdm(zip(text_embed,image_embed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df[\"single_pair_cosine\"] = single_pair_cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.to_csv(\"../data/train_data_v2.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
