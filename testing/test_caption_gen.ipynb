{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ftfy in /home/ubuntu/anaconda3/lib/python3.9/site-packages (6.1.1)\n",
      "Requirement already satisfied: regex in /home/ubuntu/anaconda3/lib/python3.9/site-packages (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/lib/python3.9/site-packages (4.62.3)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/lib/python3.9/site-packages (1.11.0+cu113)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from ftfy) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torch) (4.1.1)\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-n8sdqrio\n",
      "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-n8sdqrio\n",
      "  Resolved https://github.com/openai/CLIP.git to commit b46f5ac7587d2e1862f8b7b1573179d80dcdd620\n",
      "Requirement already satisfied: ftfy in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from clip==1.0) (6.1.1)\n",
      "Requirement already satisfied: regex in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from clip==1.0) (2021.8.3)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from clip==1.0) (4.62.3)\n",
      "Requirement already satisfied: torch in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from clip==1.0) (1.11.0+cu113)\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from clip==1.0) (0.12.0+cu113)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from ftfy->clip==1.0) (0.2.5)\n",
      "Requirement already satisfied: typing-extensions in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torch->clip==1.0) (4.1.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision->clip==1.0) (2.26.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision->clip==1.0) (1.20.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from torchvision->clip==1.0) (8.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->torchvision->clip==1.0) (3.2)\n",
      "Requirement already satisfied: pytorch-lightning in /home/ubuntu/anaconda3/lib/python3.9/site-packages (1.6.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (4.62.3)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (0.7.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (1.20.3)\n",
      "Requirement already satisfied: torch>=1.8.* in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (1.11.0+cu113)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (2.8.0)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (2021.8.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (21.0)\n",
      "Requirement already satisfied: pyDeprecate<0.4.0,>=0.3.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pytorch-lightning) (0.3.2)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.8.1)\n",
      "Requirement already satisfied: requests in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from packaging>=17.0->pytorch-lightning) (3.0.4)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.44.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.6.3)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.6)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.20.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.0.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (58.0.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.6.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.0)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning) (1.16.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (5.0.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (4.8.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.6.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (6.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (4.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/anaconda3/lib/python3.9/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy regex tqdm torch\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/model\")\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from model.aliproduct_model import ALIPRODUCT_CLIP\n",
    "from model.aliproduct_blip_model import ALIPRODUCT_BLIP\n",
    "from model.dataset import ALIPRODUCT_DATASET,prepare_data\n",
    "from model.CONFIG import CONFIG\n",
    "import faiss\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(CONFIG.global_random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv(\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/data/val_data_gen_cap.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_test = [c for c in clean_df.columns.values if \"caption\" in c ]\n",
    "# col_to_test = [\"caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caption', 'gen_caption_ft', 'gen_caption_base']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from /home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/Retrieval_aliproduct2_filtered/save_checkpoint_4.pth\n"
     ]
    }
   ],
   "source": [
    "image_size = 384\n",
    "model_url = '/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/model/BLIP/output/Retrieval_aliproduct2_filtered/save_checkpoint_4.pth'\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "        transforms.Resize((image_size,image_size),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "        ]) \n",
    "clip_model =ALIPRODUCT_BLIP(model_url,image_size,vit='base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature(caption_col):\n",
    "    test_loader,df = prepare_data(\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/data/val_data_gen_cap.csv\",\n",
    "    CONFIG.test_image_data_dir,CONFIG.test_image_data_folder\n",
    "    ,CONFIG.test_image_col,caption_col,CONFIG.test_batch_size,preprocess,CONFIG.global_random_state,test=True,tokenize=False)\n",
    "    trainer = Trainer(gpus=1)\n",
    "    pred = trainer.predict(clip_model,test_loader)\n",
    "    full_pred = tuple(map(torch.concat, zip(*pred)))\n",
    "    image_embed,text_embed = full_pred\n",
    "    faiss_index = faiss.IndexFlatIP(256)\n",
    "    print(image_embed.numpy().astype(np.float32).shape)\n",
    "    faiss_index.add(image_embed.numpy().astype(np.float32))\n",
    "    top5_k_e,top5_k_y_pred = faiss_index.search(text_embed.numpy().astype(np.float32),5)\n",
    "    top10_k_e,top10_k_y_pred = faiss_index.search(text_embed.numpy().astype(np.float32),10)\n",
    "    top5_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top5_k_y_pred)])\n",
    "    print(caption_col,\"num correct pred\",sum(top5_preds))\n",
    "    top10_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top10_k_y_pred)])\n",
    "    print(caption_col,\"num correct pred\",sum(top10_preds))\n",
    "\n",
    "    top5_acc = top5_preds[top5_preds ==1].shape[0] / top5_preds.shape[0]\n",
    "    top10_acc = top10_preds[top10_preds ==1].shape[0] / top10_preds.shape[0]\n",
    "    return top5_acc,top10_acc,text_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def topk_accuracy(y_true, pred, label_map, k,threshold):\n",
    "#     pred[pred < threshold] = 0\n",
    "#     pred_count = []\n",
    "#     remap = np.vectorize(lambda x: label_map[x][\"label\"])\n",
    "#     #get top k values\n",
    "#     top_k_pred = remap(pred.argsort(axis=1)[:, -k:][:, ::-1])\n",
    "#     # append 1 if true label is in top k values else append 0\n",
    "#     for i in range(len(y_true)):\n",
    "#         if y_true[i] in top_k_pred[i]:\n",
    "#             pred_count.append(1)\n",
    "#         else:\n",
    "#             pred_count.append(0)\n",
    "#     return sum(pred_count)/len(pred_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 313/313 [18:17<00:00,  3.51s/it]\n",
      "(50000, 256)\n",
      "caption num correct pred 25291\n",
      "caption num correct pred 31624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 313/313 [18:11<00:00,  3.49s/it]\n",
      "(50000, 256)\n",
      "gen_caption_ft num correct pred 17053\n",
      "gen_caption_ft num correct pred 23198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 313/313 [18:08<00:00,  3.48s/it]\n",
      "(50000, 256)\n",
      "gen_caption_base num correct pred 7224\n",
      "gen_caption_base num correct pred 9946\n"
     ]
    }
   ],
   "source": [
    "col_name = []\n",
    "top5_acc_preds = []\n",
    "top10_acc_preds = []\n",
    "embedding_dict  = {}\n",
    "for col in col_to_test:\n",
    "    top5_acc,top10_acc,text_embed = test_feature(col)\n",
    "    col_name.append(col)\n",
    "    top5_acc_preds.append(top5_acc)\n",
    "    top10_acc_preds.append(top10_acc)\n",
    "    embedding_dict[col] =text_embed\n",
    "pred_df = {\"caption_gen_method\":col_name,\"top_5\":top5_acc_preds,\"top_10\":top10_acc_preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption_gen_method</th>\n",
       "      <th>top_5</th>\n",
       "      <th>top_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caption</td>\n",
       "      <td>0.50582</td>\n",
       "      <td>0.63248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gen_caption_ft</td>\n",
       "      <td>0.34106</td>\n",
       "      <td>0.46396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gen_caption_base</td>\n",
       "      <td>0.14448</td>\n",
       "      <td>0.19892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  caption_gen_method    top_5   top_10\n",
       "0            caption  0.50582  0.63248\n",
       "1     gen_caption_ft  0.34106  0.46396\n",
       "2   gen_caption_base  0.14448  0.19892"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_to_txt_acc(query,key,df):\n",
    "    faiss_index = faiss.IndexFlatIP(256)\n",
    "    faiss_index.add(key.numpy().astype(np.float32))\n",
    "    top5_k_e,top5_k_y_pred = faiss_index.search(query.numpy().astype(np.float32),5)\n",
    "    top10_k_e,top10_k_y_pred = faiss_index.search(query.numpy().astype(np.float32),10)\n",
    "    top5_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top5_k_y_pred)])\n",
    "    top10_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top10_k_y_pred)])\n",
    "    top5_acc = top5_preds[top5_preds ==1].shape[0] / top5_preds.shape[0]\n",
    "    top10_acc = top10_preds[top10_preds ==1].shape[0] / top10_preds.shape[0]\n",
    "    return top5_acc,top10_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_txt = []\n",
    "top5_acc_preds_txt = []\n",
    "top10_acc_preds_txt = []\n",
    "for col in embedding_dict.keys():\n",
    "    top5_acc,top10_acc = txt_to_txt_acc(embedding_dict[col],embedding_dict[\"caption\"],clean_df)\n",
    "    col_name_txt.append(col)\n",
    "    top5_acc_preds_txt.append(top5_acc)\n",
    "    top10_acc_preds_txt.append(top10_acc)\n",
    "pred_df_txt = {\"caption_gen_method\":col_name_txt,\"top_5_txt_2_txt\":top5_acc_preds_txt,\"top_10_txt_2_txt\":top10_acc_preds_txt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption_gen_method</th>\n",
       "      <th>top_5_txt_2_txt</th>\n",
       "      <th>top_10_txt_2_txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caption</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gen_caption_ft</td>\n",
       "      <td>0.1780</td>\n",
       "      <td>0.26222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gen_caption_base</td>\n",
       "      <td>0.0497</td>\n",
       "      <td>0.07656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  caption_gen_method  top_5_txt_2_txt  top_10_txt_2_txt\n",
       "0            caption           1.0000           1.00000\n",
       "1     gen_caption_ft           0.1780           0.26222\n",
       "2   gen_caption_base           0.0497           0.07656"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df_txt = pd.DataFrame(pred_df_txt)\n",
    "pred_df_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
