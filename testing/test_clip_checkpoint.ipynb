{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm torch\n",
    "!pip install git+https://github.com/openai/CLIP.git\n",
    "!pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path+\"/model\")\n",
    "import clip\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from sklearn.metrics import top_k_accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import os\n",
    "import pickle\n",
    "from model.aliproduct_model import ALIPRODUCT_CLIP\n",
    "from model.dataset import ALIPRODUCT_DATASET,prepare_data\n",
    "from model.CONFIG import CONFIG\n",
    "import faiss\n",
    "import open_clip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('RN50', 'openai'),\n",
       " ('RN50', 'yfcc15m'),\n",
       " ('RN50', 'cc12m'),\n",
       " ('RN50-quickgelu', 'openai'),\n",
       " ('RN50-quickgelu', 'yfcc15m'),\n",
       " ('RN50-quickgelu', 'cc12m'),\n",
       " ('RN101', 'openai'),\n",
       " ('RN101', 'yfcc15m'),\n",
       " ('RN101-quickgelu', 'openai'),\n",
       " ('RN101-quickgelu', 'yfcc15m'),\n",
       " ('RN50x4', 'openai'),\n",
       " ('RN50x16', 'openai'),\n",
       " ('ViT-B-32', 'openai'),\n",
       " ('ViT-B-32', 'laion400m_e31'),\n",
       " ('ViT-B-32', 'laion400m_e32'),\n",
       " ('ViT-B-32', 'laion400m_avg'),\n",
       " ('ViT-B-32-quickgelu', 'openai'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e31'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_e32'),\n",
       " ('ViT-B-32-quickgelu', 'laion400m_avg'),\n",
       " ('ViT-B-16', 'openai'),\n",
       " ('ViT-B-16', 'laion400m_e31'),\n",
       " ('ViT-B-16', 'laion400m_e32'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e31'),\n",
       " ('ViT-B-16-plus-240', 'laion400m_e32'),\n",
       " ('ViT-L-14', 'openai'),\n",
       " ('ViT-L-14-336', 'openai')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "open_clip.list_pretrained()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size / architecture of clip model \n",
    "clip_model_size = \"ViT-B-32\"\n",
    "\n",
    "\n",
    "#model checkpoint\n",
    "checkpoint= \"openai\"\n",
    "\n",
    "#Edit this to specify the augmented/original captions you want to test the model checkpoints on \n",
    "caption_to_test =  [\"caption\"]\n",
    "\n",
    "\n",
    "#path to validation data after preporcessing into csv file format \n",
    "dataframe_path = \"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/data/val_data_prompt_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model,_,preprocess = open_clip.create_model_and_transforms(clip_model_size,pretrained=checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = ALIPRODUCT_CLIP(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature(caption_col):\n",
    "    test_loader,df = prepare_data(\"/home/ubuntu/Desktop/CVPR 2022 AliProducts Challenge/code/data/val_data_prompt_clean.csv\",\n",
    "    CONFIG.test_image_data_dir,CONFIG.test_image_data_folder\n",
    "    ,CONFIG.test_image_col,caption_col,CONFIG.test_batch_size,preprocess,CONFIG.global_random_state,test=True)\n",
    "    trainer = Trainer(gpus=1)\n",
    "    pred = trainer.predict(clip_model,test_loader)\n",
    "    full_pred = tuple(map(torch.concat, zip(*pred)))\n",
    "    image_embed,text_embed = full_pred\n",
    "    faiss_index = faiss.IndexFlatIP(image_embed.shape[1])\n",
    "    faiss_index_gpu = faiss.index_cpu_to_all_gpus(faiss_index)\n",
    "    faiss_index_gpu.add(image_embed.numpy().astype(np.float32))\n",
    "    top5_k_e,top5_k_y_pred = faiss_index_gpu.search(text_embed.numpy().astype(np.float32),5)\n",
    "    top10_k_e,top10_k_y_pred = faiss_index_gpu.search(text_embed.numpy().astype(np.float32),10)\n",
    "    top5_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top5_k_y_pred)])\n",
    "    print(caption_col,\"num correct pred\",sum(top5_preds))\n",
    "    top10_preds = np.array([1 if y_true in y_pred else 0 for y_true,y_pred in zip(df.index.values,top10_k_y_pred)])\n",
    "    print(caption_col,\"num correct pred\",sum(top10_preds))\n",
    "\n",
    "    top5_acc = top5_preds[top5_preds ==1].shape[0] / top5_preds.shape[0]\n",
    "    top10_acc = top10_preds[top10_preds ==1].shape[0] / top10_preds.shape[0]\n",
    "    return top5_acc,top10_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def topk_accuracy(y_true, pred, label_map, k,threshold):\n",
    "#     pred[pred < threshold] = 0\n",
    "#     pred_count = []\n",
    "#     remap = np.vectorize(lambda x: label_map[x][\"label\"])\n",
    "#     #get top k values\n",
    "#     top_k_pred = remap(pred.argsort(axis=1)[:, -k:][:, ::-1])\n",
    "#     # append 1 if true label is in top k values else append 0\n",
    "#     for i in range(len(y_true)):\n",
    "#         if y_true[i] in top_k_pred[i]:\n",
    "#             pred_count.append(1)\n",
    "#         else:\n",
    "#             pred_count.append(0)\n",
    "#     return sum(pred_count)/len(pred_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name = []\n",
    "top5_acc_preds = []\n",
    "top10_acc_preds = []\n",
    "for col in caption_to_test:\n",
    "    top5_acc,top10_acc = test_feature(col)\n",
    "    col_name.append(col)\n",
    "    top5_acc_preds.append(top5_acc)\n",
    "    top10_acc_preds.append(top10_acc)\n",
    "pred_df = {\"caption_clean_method\":col_name,\"top_5\":top5_acc_preds,\"top_10\":top10_acc_preds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clip_model = ALIPRODUCT_CLIP(model)\n",
    "# checkpoint = torch.load(\"../model_weights/ViT_large.pt\")\n",
    "# for key in list(checkpoint[\"state_dict\"].keys()):\n",
    "#     checkpoint[\"state_dict\"][key.replace('module.', 'model.')] = checkpoint[\"state_dict\"].pop(key)\n",
    "# clip_model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top5_acc_trained_preds = []\n",
    "# top10_acc_trained_preds = []\n",
    "# for col in col_to_test:\n",
    "#     top5_acc,top10_acc = test_feature(col)\n",
    "#     top5_acc_trained_preds.append(top5_acc)\n",
    "#     top10_acc_trained_preds.append(top10_acc)\n",
    "# pred_df[\"top_5_trained\"] = top5_acc_trained_preds\n",
    "# pred_df[\"top_10_trained\"] = top10_acc_trained_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "caf1c2fcf97217de91eafa76b907d50f9ea378f5ffbee7f571142d119bb6a771"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
